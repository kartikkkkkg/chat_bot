{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import re\nimport tensorflow as tf\nimport numpy as np\nfrom tensorflow.keras.utils import pad_sequences\n\nfrom keras.layers import Input\n\nfrom keras.models import Model\nfrom keras.layers import Dense, Embedding, LSTM\n\nlines = open('/kaggle/input/chat-bot/Chatbot/movie_lines.txt', encoding='utf-8',errors='ignore').read().split('\\n')\n\nconvers = open('/kaggle/input/chat-bot/Chatbot/movie_conversations.txt', encoding='utf-8',errors='ignore').read().split('\\n')\n\n\nexchn = []\nfor conver in convers:\n    exchn.append(conver.split(' +++$+++ ')[-1][1:-1].replace(\"'\", \" \").replace(\",\",\"\").split())\n\ndiag = {}\nfor line in lines:\n    diag[line.split(' +++$+++ ')[0]] = line.split(' +++$+++ ')[-1]\n\n## delete\ndel(lines, convers, conver, line)   \n\nquestions = []\nanswers = []\n\nfor conver in exchn:\n    for i in range(len(conver) - 1):\n        questions.append(diag[conver[i]])\n        answers.append(diag[conver[i+1]])\n\n## delete\ndel(diag, exchn, conver, i)\n\n\n###############################\n#        max_len = 13         #\n###############################\n\nsorted_ques = []\nsorted_ans = []\nfor i in range(len(questions)):\n    if len(questions[i]) < 13:\n        sorted_ques.append(questions[i])\n        sorted_ans.append(answers[i])\n\n\n\ndef clean_text(txt):\n    txt = txt.lower()\n    txt = re.sub(r\"i'm\", \"i am\", txt)\n    txt = re.sub(r\"he's\", \"he is\", txt)\n    txt = re.sub(r\"she's\", \"she is\", txt)\n    txt = re.sub(r\"that's\", \"that is\", txt)\n    txt = re.sub(r\"what's\", \"what is\", txt)\n    txt = re.sub(r\"where's\", \"where is\", txt)\n    txt = re.sub(r\"\\'ll\", \" will\", txt)\n    txt = re.sub(r\"\\'ve\", \" have\", txt)\n    txt = re.sub(r\"\\'re\", \" are\", txt)\n    txt = re.sub(r\"\\'d\", \" would\", txt)\n    txt = re.sub(r\"won't\", \"will not\", txt)\n    txt = re.sub(r\"can't\", \"can not\", txt)\n    txt = re.sub(r\"[^\\w\\s]\", \"\", txt)\n    return txt\n\nclean_ques = []\nclean_ans = []\n\nfor line in sorted_ques:\n    clean_ques.append(clean_text(line))\n        \nfor line in sorted_ans:\n    clean_ans.append(clean_text(line))\n\n\n\n## delete\ndel(answers, questions, line)\n\ndel(i)\n\nfor i in range(len(clean_ans)):\n    clean_ans[i] = ' '.join(clean_ans[i].split()[:11])\n\n\n\n###############################\n# data processing still...    #\n###############################\n\ndel(sorted_ans, sorted_ques)\n\n\n## trimming\nclean_ans=clean_ans[:30000]\nclean_ques=clean_ques[:30000]\n## delete\n\n\n###  count occurences ###\nword2count = {}\n\nfor line in clean_ques:\n    for word in line.split():\n        if word not in word2count:\n            word2count[word] = 1\n        else:\n            word2count[word] += 1\nfor line in clean_ans:\n    for word in line.split():\n        if word not in word2count:\n            word2count[word] = 1\n        else:\n            word2count[word] += 1\n\n## delete\ndel(word, line)\n\n\n###  remove less frequent ###\nthresh = 5\n\nvocab = {}\nword_num = 0\nfor word, count in word2count.items():\n    if count >= thresh:\n        vocab[word] = word_num\n        word_num += 1\n        \n## delete\ndel(word2count, word, count, thresh)       \ndel(word_num)        \n\n\n\nfor i in range(len(clean_ans)):\n    clean_ans[i] = '<SOS> ' + clean_ans[i] + ' <EOS>'\n\n\n\ntokens = ['<PAD>', '<EOS>', '<OUT>', '<SOS>']\nx = len(vocab)\nfor token in tokens:\n    vocab[token] = x\n    x += 1\n    \n    \n\nvocab['cameron'] = vocab['<PAD>']\nvocab['<PAD>'] = 0\n\n## delete\ndel(token, tokens) \ndel(x)\n\n### inv answers dict ###\ninv_vocab = {w:v for v, w in vocab.items()}\n\n\n\n## delete\ndel(i)\n\n\n\nencoder_inp = []\nfor line in clean_ques:\n    lst = []\n    for word in line.split():\n        if word not in vocab:\n            lst.append(vocab['<OUT>'])\n        else:\n            lst.append(vocab[word])\n        \n    encoder_inp.append(lst)\n\ndecoder_inp = []\nfor line in clean_ans:\n    lst = []\n    for word in line.split():\n        if word not in vocab:\n            lst.append(vocab['<OUT>'])\n        else:\n            lst.append(vocab[word])        \n    decoder_inp.append(lst)\n\n### delete\ndel(clean_ans, clean_ques, line, lst, word)\n\n\n\nencoder_inp = tf.keras.utils.pad_sequences(encoder_inp, 13, padding='post', truncating='post')\ndecoder_inp = tf.keras.utils.pad_sequences(decoder_inp, 13, padding='post', truncating='post')\n\n\n\n\ndecoder_final_output = []\nfor i in decoder_inp:\n    decoder_final_output.append(i[1:]) \n\ndecoder_final_output = tf.keras.utils.pad_sequences(decoder_final_output, 13, padding='post', truncating='post')\n\n\ndel(i)\n\nfrom keras.utils import to_categorical\ndecoder_final_output = to_categorical(decoder_final_output, len(vocab))\n\n\n\n\nenc_inp = Input(shape=(13, ))\ndec_inp = Input(shape=(13, ))\n\n\nVOCAB_SIZE = len(vocab)\nembed = Embedding(VOCAB_SIZE+1, output_dim=50, \n                  input_length=13,\n                  trainable=True                  \n                  )\n\n\nenc_embed = embed(enc_inp)\nenc_lstm = LSTM(400, return_sequences=True, return_state=True)\nenc_op, h, c = enc_lstm(enc_embed)\nenc_states = [h, c]\n\n\n\ndec_embed = embed(dec_inp)\ndec_lstm = LSTM(400, return_sequences=True, return_state=True)\ndec_op, _, _ = dec_lstm(dec_embed, initial_state=enc_states)\n\ndense = Dense(VOCAB_SIZE, activation='softmax')\n\ndense_op = dense(dec_op)\n\nmodel = Model([enc_inp, dec_inp], dense_op)\n\n\n\n\nmodel.compile(loss='categorical_crossentropy',metrics=['acc'],optimizer='adam')\n\nmodel.fit([encoder_inp, decoder_inp],decoder_final_output,epochs=100)\n\n\n\nenc_model = Model([enc_inp], enc_states)\n\n\n\n# decoder Model\ndecoder_state_input_h = Input(shape=(400,))\ndecoder_state_input_c = Input(shape=(400,))\n\ndecoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n\n\ndecoder_outputs, state_h, state_c = dec_lstm(dec_embed , \n                                    initial_state=decoder_states_inputs)\n\n\ndecoder_states = [state_h, state_c]\n\n\ndec_model = Model([dec_inp]+ decoder_states_inputs,\n                                      [decoder_outputs]+ decoder_states)\n\n\n\n\n\nprint(\"##########################################\")\nprint(\"#       start chatting ver. 1.0          #\")\nprint(\"##########################################\")\n\n\nprepro1 = \"\"\nwhile prepro1 != 'q':\n    prepro1  = input(\"you : \")\n    ## prepro1 = \"Hello\"\n\n    prepro1 = clean_text(prepro1)\n    ## prepro1 = \"hello\"\n\n    prepro = [prepro1]\n    ## prepro1 = [\"hello\"]\n\n    txt = []\n    for x in prepro:\n        # x = \"hello\"\n        lst = []\n        for y in x.split():\n            ## y = \"hello\"\n            try:\n                lst.append(vocab[y])\n                ## vocab['hello'] = 454\n            except:\n                lst.append(vocab['<OUT>'])\n        txt.append(lst)\n\n    ## txt = [[454]]\n    txt = pad_sequences(txt, 13, padding='post')\n\n    ## txt = [[454,0,0,0,.........13]]\n\n    stat = enc_model.predict( txt )\n\n    empty_target_seq = np.zeros( ( 1 , 1) )\n     ##   empty_target_seq = [0]\n\n\n    empty_target_seq[0, 0] = vocab['<SOS>']\n    ##    empty_target_seq = [255]\n\n    stop_condition = False\n    decoded_translation = ''\n\n    while not stop_condition :\n\n        dec_outputs , h, c= dec_model.predict([ empty_target_seq] + stat )\n        decoder_concat_input = dense(dec_outputs)\n        ## decoder_concat_input = [0.1, 0.2, .4, .0, ...............]\n\n        sampled_word_index = np.argmax( decoder_concat_input[0, -1, :] )\n        ## sampled_word_index = [2]\n\n        sampled_word = inv_vocab[sampled_word_index] + ' '\n\n        ## inv_vocab[2] = 'hi'\n        ## sampled_word = 'hi '\n\n        if sampled_word != '<EOS> ':\n            decoded_translation += sampled_word  \n\n        if sampled_word == '<EOS> ' or len(decoded_translation.split()) > 13:\n            stop_condition = True \n\n        empty_target_seq = np.zeros( ( 1 , 1 ) )  \n        empty_target_seq[ 0 , 0 ] = sampled_word_index\n        ## <SOS> - > hi\n        ## hi --> <EOS>\n        stat = [h, c]  \n\n    print(\"chatbot attention : \", decoded_translation )\n    print(\"==============================================\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-03-10T11:34:07.397673Z","iopub.execute_input":"2023-03-10T11:34:07.398877Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Epoch 1/100\n938/938 [==============================] - 151s 155ms/step - loss: 3.0932 - acc: 0.4946\nEpoch 2/100\n938/938 [==============================] - 146s 156ms/step - loss: 2.7367 - acc: 0.5320\nEpoch 3/100\n938/938 [==============================] - 154s 164ms/step - loss: 2.6056 - acc: 0.5423\nEpoch 4/100\n938/938 [==============================] - 154s 164ms/step - loss: 2.5295 - acc: 0.5476\nEpoch 5/100\n938/938 [==============================] - 156s 166ms/step - loss: 2.4712 - acc: 0.5509\nEpoch 6/100\n938/938 [==============================] - 152s 163ms/step - loss: 2.4234 - acc: 0.5534\nEpoch 7/100\n938/938 [==============================] - 160s 170ms/step - loss: 2.3757 - acc: 0.5560\nEpoch 8/100\n938/938 [==============================] - 153s 163ms/step - loss: 2.3321 - acc: 0.5581\nEpoch 9/100\n938/938 [==============================] - 153s 163ms/step - loss: 2.2880 - acc: 0.5604\nEpoch 10/100\n938/938 [==============================] - 157s 168ms/step - loss: 2.2427 - acc: 0.5622\nEpoch 11/100\n938/938 [==============================] - 156s 167ms/step - loss: 2.1952 - acc: 0.5648\nEpoch 12/100\n938/938 [==============================] - 153s 163ms/step - loss: 2.1491 - acc: 0.5676\nEpoch 13/100\n938/938 [==============================] - 151s 161ms/step - loss: 2.1042 - acc: 0.5710\nEpoch 14/100\n938/938 [==============================] - 155s 165ms/step - loss: 2.0607 - acc: 0.5741\nEpoch 15/100\n938/938 [==============================] - 150s 160ms/step - loss: 2.0174 - acc: 0.5783\nEpoch 16/100\n938/938 [==============================] - 152s 162ms/step - loss: 1.9759 - acc: 0.5827\nEpoch 17/100\n938/938 [==============================] - 148s 158ms/step - loss: 1.9365 - acc: 0.5873\nEpoch 18/100\n938/938 [==============================] - 147s 157ms/step - loss: 1.8967 - acc: 0.5931\nEpoch 19/100\n938/938 [==============================] - 150s 160ms/step - loss: 1.8580 - acc: 0.5984\nEpoch 20/100\n938/938 [==============================] - 149s 159ms/step - loss: 1.8208 - acc: 0.6037\nEpoch 21/100\n  7/938 [..............................] - ETA: 2:23 - loss: 1.6494 - acc: 0.6391","output_type":"stream"},{"text":"IOPub message rate exceeded.\nThe notebook server will temporarily stop sending output\nto the client in order to avoid crashing it.\nTo change this limit, set the config variable\n`--NotebookApp.iopub_msg_rate_limit`.\n\nCurrent values:\nNotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\nNotebookApp.rate_limit_window=3.0 (secs)\n\n","name":"stderr","output_type":"stream"},{"name":"stdout","text":"938/938 [==============================] - 150s 160ms/step - loss: 1.7490 - acc: 0.6151\nEpoch 23/100\n938/938 [==============================] - 150s 159ms/step - loss: 1.7148 - acc: 0.6210\nEpoch 24/100\n938/938 [==============================] - 147s 157ms/step - loss: 1.6811 - acc: 0.6267\nEpoch 25/100\n938/938 [==============================] - 147s 157ms/step - loss: 1.6482 - acc: 0.6330\nEpoch 26/100\n938/938 [==============================] - 146s 156ms/step - loss: 1.6163 - acc: 0.6390\nEpoch 27/100\n938/938 [==============================] - 146s 156ms/step - loss: 1.5848 - acc: 0.6445\nEpoch 28/100\n938/938 [==============================] - 148s 157ms/step - loss: 1.5540 - acc: 0.6507\nEpoch 29/100\n938/938 [==============================] - 149s 158ms/step - loss: 1.5235 - acc: 0.6566\nEpoch 30/100\n938/938 [==============================] - 147s 157ms/step - loss: 1.4943 - acc: 0.6628\nEpoch 31/100\n938/938 [==============================] - 151s 161ms/step - loss: 1.4646 - acc: 0.6682\nEpoch 32/100\n938/938 [==============================] - 193s 206ms/step - loss: 1.4369 - acc: 0.6739\nEpoch 33/100\n938/938 [==============================] - 193s 206ms/step - loss: 1.4086 - acc: 0.6796\nEpoch 34/100\n938/938 [==============================] - 191s 204ms/step - loss: 1.3814 - acc: 0.6857\nEpoch 35/100\n938/938 [==============================] - 192s 204ms/step - loss: 1.3547 - acc: 0.6913\nEpoch 36/100\n938/938 [==============================] - 191s 204ms/step - loss: 1.3284 - acc: 0.6969\nEpoch 37/100\n938/938 [==============================] - 189s 202ms/step - loss: 1.3026 - acc: 0.7027\nEpoch 38/100\n938/938 [==============================] - 188s 201ms/step - loss: 1.2774 - acc: 0.7083\nEpoch 39/100\n938/938 [==============================] - 190s 203ms/step - loss: 1.2523 - acc: 0.7139\nEpoch 40/100\n938/938 [==============================] - 189s 201ms/step - loss: 1.2285 - acc: 0.7185\nEpoch 41/100\n938/938 [==============================] - 187s 200ms/step - loss: 1.2046 - acc: 0.7242\nEpoch 42/100\n938/938 [==============================] - 181s 192ms/step - loss: 1.1823 - acc: 0.7297\nEpoch 43/100\n938/938 [==============================] - 148s 158ms/step - loss: 1.1599 - acc: 0.7345\nEpoch 44/100\n938/938 [==============================] - 150s 160ms/step - loss: 1.1379 - acc: 0.7390\nEpoch 45/100\n938/938 [==============================] - 146s 156ms/step - loss: 1.1162 - acc: 0.7441\nEpoch 46/100\n938/938 [==============================] - 148s 157ms/step - loss: 1.0959 - acc: 0.7488\nEpoch 47/100\n938/938 [==============================] - 150s 159ms/step - loss: 1.0757 - acc: 0.7537\nEpoch 48/100\n938/938 [==============================] - 148s 158ms/step - loss: 1.0563 - acc: 0.7578\nEpoch 49/100\n938/938 [==============================] - 153s 163ms/step - loss: 1.0359 - acc: 0.7625\nEpoch 50/100\n938/938 [==============================] - 151s 161ms/step - loss: 1.0179 - acc: 0.7665\nEpoch 51/100\n938/938 [==============================] - 151s 161ms/step - loss: 1.0009 - acc: 0.7712\nEpoch 52/100\n938/938 [==============================] - 153s 163ms/step - loss: 0.9826 - acc: 0.7745\nEpoch 53/100\n938/938 [==============================] - 153s 164ms/step - loss: 0.9640 - acc: 0.7797\nEpoch 54/100\n938/938 [==============================] - 151s 161ms/step - loss: 0.9480 - acc: 0.7830\nEpoch 55/100\n938/938 [==============================] - 150s 160ms/step - loss: 0.9325 - acc: 0.7871\nEpoch 56/100\n938/938 [==============================] - 154s 165ms/step - loss: 0.9161 - acc: 0.7902\nEpoch 57/100\n938/938 [==============================] - 149s 159ms/step - loss: 0.9010 - acc: 0.7940\nEpoch 58/100\n938/938 [==============================] - 149s 159ms/step - loss: 0.8859 - acc: 0.7981\nEpoch 59/100\n938/938 [==============================] - 148s 158ms/step - loss: 0.8717 - acc: 0.8010\nEpoch 60/100\n938/938 [==============================] - 151s 161ms/step - loss: 0.8585 - acc: 0.8040\nEpoch 61/100\n938/938 [==============================] - 191s 203ms/step - loss: 0.8440 - acc: 0.8077\nEpoch 62/100\n938/938 [==============================] - 191s 204ms/step - loss: 0.8301 - acc: 0.8114\nEpoch 63/100\n938/938 [==============================] - 191s 203ms/step - loss: 0.8189 - acc: 0.8132\nEpoch 64/100\n938/938 [==============================] - 191s 203ms/step - loss: 0.8060 - acc: 0.8169\nEpoch 65/100\n181/938 [====>.........................] - ETA: 2:32 - loss: 0.7618 - acc: 0.8294","output_type":"stream"}]}]}